{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enron Email Investigation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Data for project_** \n",
    "\n",
    "The provided dataset is compiled from two sources of Enron company data: financial records for employees, and email exchanges within the company. When combined this gives us a relatively good bredth of features to investigate. As the goal of this project is essentially a simple classification problem - i.e., is a given employee a POI (Person Of Interest), or not - it is particularly well-suited to machine learning, as it can help to identify patterns in the email exchanges etc that can aid in the identification of POIs!  \n",
    "\n",
    "There were a few outliers that had to be removed prior to the analysis. One was the 'TOTAL' of all employees' financial data, while the other was a travel agency that was listed as an employee. This dataset is also very skewed, many more of one class (non-POI's) than the other (POI's), which can provide certain challenges particularly when trying to validate the chosen models.  \n",
    "\n",
    "**_Feature selection and performance_**  \n",
    "\n",
    "A new feature was created - the ratio of an employees stock that was exercised over their total stock allocation. Intuitively I thought that this would provide a good measure of those who knew that the company was struggling or imperiled, and hence sold off their stock before it crashed. If that were the case then it would indicate that such an individual had knowledge of the company's fraudulent activity, and hence may be a POI. I did a quick test of this feature by seeing if the POI/non-POI ratio was different from the population ratio when a cut-off value for the new feature was used. I arbitrarily used 60% as the proportion of stock sold-off as a mark of suspicious activity. While the ratio was higher (18% vs 12.5%), the small sample sizes made for a lot of variance, and so this new feature may or may not be a strong indicator of POI activity. Its true application would need to be assessed as it relates with other features in the dataset. \n",
    "\n",
    "Feature-analysis performed using SelectKBest and PCA showed that financial features performed well. The best email-based features appeared to be the 'from_this_person_to_poi' and 'from_poi_to_this_person' ones, which would match well with our expectations. However, given the quite wide feature-space I chose to input all of the available features and then perform a SelectKBest to get the features to use. I ran a GridSearchCV over the range 1 - 19 features, with the search settling on selecting 16 features for the analysis. Scaling was not used as the DecisionTreeClassifier algorithm is not geometrically-dependent.  \n",
    "\n",
    "**_Choice of algorithm_**\n",
    "\n",
    "Algorithms attempted include: GaussianNB, DecisionTreeClassifier (DT), SVM, and KNearestNeighbors . After preliminary tests on the dataset with unrefined parameters, the GaussianNB, DT, and KNN algorithms all gave relatively good performance, while the SVM had quite poor performance. For each, the precision/recall values varied strongly depending on the algorithm. I selected to use a DT as *both* the precision and recall values were already >0.3 using unoptimized settings. Also, it has a large number of parameters to 'tune' compared with GaussianNB, allowing the possibility for further improving the metric scores.\n",
    "\n",
    "**_Parameter tuning_**\n",
    "\n",
    "Parameter tuning is the practise of tweaking the settings of the classifier algorithm to find the optimum performance. This can be done manually, however it is very difficult to do this in a systematic and/or reproducible way. I used GridSearchCV to automatically test a large parameter-space to find the best-performing parameter settings for my DT classifier. Although using a wide range of parameter values can result in long computational run times, this process can be divided into two steps: coarse tuning, and fine tuning. In the first, a small range of vastly disparate parameter settings are attempted (for parameters with numerical inputs). Next, the search can be performed again on a small range of narrowly dispersed values surrounding the value first found in the coarse tune.\n",
    "\n",
    "For some parameters, the default settings were found to operate best (e.g. 'max_depth'), and so those were left out from the grid search in subsequent runs. Perhaps the most important parameter that needed to be tuned in this analysis was the 'class_weight', as the default is 'None', but the 'balanced' setting is thought to be much better for skewed classes (such as the POI/non-POI set).  \n",
    "\n",
    "**_Model validation_**  \n",
    "\n",
    "Validation is the processing of testing your model on unseen data. If validation is performed on data that was used to train the model, or somehow or another linked to the training data, then 'overfitting' can occur resulting in model outputs with a high variance error.  With small datasets (such as this one) validation can be difficult. The StrattifiedShuffleSplit 'cross-validation' process was therefore employed, as this performs multiple 'folds' of the dataset split randomly into training/testing sets and generates a model based on the averaged results from each fold. Additionally, the 'stratification' refers to each fold containing a representative class balance. This means that for  classification tasks using imbalanced datasets such as this one it accounts for the class imbalance when creating each fold, giving splits with approximately the same ratio of POIs/non-POIs in each fold, and hence ensuring more accurate model predictions. This is important for maximising model validation from a small dataset.  \n",
    "\n",
    "**_Evaluation metrics_**  \n",
    "\n",
    "Precision and recall were used as the primary evaluation metrics for this analysis, as these are good for use in cases where the classes are imbalanced. 'Precision' relates to a low 'false positive' rate, while 'recall' relates to a low 'false negative' rate. When tuning the algorithm parameters using the grid search, the 'F1' metric was used as the metric to be optimized during the search, because this is the harmonic mean of precision and recall.  \n",
    "\n",
    "As the outcome of the final algorithm gave a recall of 0.64, and a precision of 0.34, this indicates that the model is flagging most of the POIs (high RECALL), but also flagging a lot of non-POIs as POIs (low PRECISION). For the context of investigating the Enron employees this situation (i.e. high recall/low precision) may be preferable over the reverse (low precision/high recall), as at least most of the POIs are being flagged. Moreover, non-POIs that have been flagged as POIs can be investigated more closely to determine their true 'classification' before any prosecution is carried out.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset investigation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data into the 'data_dict' obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/TBD/anaconda/lib/python2.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pickle\n",
    "sys.path.append(\"../tools/\")\n",
    "\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester import dump_classifier_and_data\n",
    "\n",
    "### Task 1: Select what features you'll use.\n",
    "### features_list is a list of strings, each of which is a feature name.\n",
    "### The first feature must be \"poi\".\n",
    "features_list = ['poi', 'salary', 'scaled_stock_exercised', 'from_poi_to_this_person', \\\n",
    " 'from_this_person_to_poi', 'shared_receipt_with_poi', 'to_messages', 'from_messages', \\\n",
    " 'deferral_payments', 'total_payments', 'bonus', 'restricted_stock_deferred', 'long_term_incentive', \\\n",
    " 'loan_advances', 'deferred_income', 'total_stock_value', 'expenses', 'exercised_stock_options', \\\n",
    " 'director_fees', 'restricted_stock']\n",
    "\n",
    "### Load the dictionary containing the dataset\n",
    "with open(\"final_project_dataset.pkl\", \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After loading the pickle file into the dictionary 'data_dict', want to check out some characteristics of this 'dict-of-dicts'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find number of data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146\n"
     ]
    }
   ],
   "source": [
    "print len(data_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The length of the data_dict dictionary is 146. This is the number of Enron EMPLOYEES that we have (at least partial) data for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example employee entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'salary': 420636, 'to_messages': 905, 'deferral_payments': 'NaN', 'total_payments': 505050, 'exercised_stock_options': 19794175, 'bonus': 1750000, 'restricted_stock': 2748364, 'shared_receipt_with_poi': 864, 'restricted_stock_deferred': 'NaN', 'total_stock_value': 22542539, 'expenses': 46950, 'loan_advances': 'NaN', 'from_messages': 18, 'other': 174839, 'from_this_person_to_poi': 4, 'poi': True, 'director_fees': 'NaN', 'deferred_income': -3504386, 'long_term_incentive': 1617011, 'email_address': 'ken.rice@enron.com', 'from_poi_to_this_person': 42}\n"
     ]
    }
   ],
   "source": [
    "print data_dict['RICE KENNETH D']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find number of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n"
     ]
    }
   ],
   "source": [
    "num_features = 0\n",
    "# use 'KENNETH D RICE' as example employee\n",
    "for feature in data_dict['RICE KENNETH D']:\n",
    "    num_features += 1\n",
    "\n",
    "print num_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 21 features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Salary records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of Enron employee salaries we have records for: 95\n",
      "Proportion of dataset with salary info: 0.650684931507\n"
     ]
    }
   ],
   "source": [
    "# find proportion of employee 'salary' figures for which we have data\n",
    "count = 0\n",
    "for employee in data_dict:\n",
    "    for key in data_dict[employee]:\n",
    "        if key == 'salary':\n",
    "            if data_dict[employee][key] != 'NaN':\n",
    "                count += 1\n",
    "            #print data_dict[employee][key]\n",
    "\n",
    "print \"No. of Enron employee salaries we have records for:\", count\n",
    "print \"Proportion of dataset with salary info:\", float(count) / len(data_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above indicates that we have salary info for 95 Enron employees, or 65% of total employees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of POIs/non-POIs in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of Enron employee who are listed as POIs: 18\n",
      "Total no. of employees: 146\n"
     ]
    }
   ],
   "source": [
    "# find proportion of employee POIs in data set\n",
    "poi_count = 0\n",
    "non_poi_count = 0\n",
    "\n",
    "for employee in data_dict:\n",
    "    for key in data_dict[employee]:\n",
    "        if key == 'poi':\n",
    "            if data_dict[employee][key]:\n",
    "                poi_count += 1\n",
    "            else:\n",
    "                non_poi_count += 1\n",
    "\n",
    "print \"No. of Enron employee who are listed as POIs:\", poi_count\n",
    "print \"Total no. of employees:\", poi_count + non_poi_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, for our algorithm our 'class alocation' will be skewed. There are 128 non-POIs, and only 18 POIs. This may require special attention or preprocessing during the modelling step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Employees flagged as POIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HANNON KEVIN P\n",
      "COLWELL WESLEY\n",
      "RIEKER PAULA H\n",
      "KOPPER MICHAEL J\n",
      "SHELBY REX\n",
      "DELAINEY DAVID W\n",
      "LAY KENNETH L\n",
      "BOWEN JR RAYMOND M\n",
      "BELDEN TIMOTHY N\n",
      "FASTOW ANDREW S\n",
      "CALGER CHRISTOPHER F\n",
      "RICE KENNETH D\n",
      "SKILLING JEFFREY K\n",
      "YEAGER F SCOTT\n",
      "HIRKO JOSEPH\n",
      "KOENIG MARK E\n",
      "CAUSEY RICHARD A\n",
      "GLISAN JR BEN F\n"
     ]
    }
   ],
   "source": [
    "for employee in data_dict:\n",
    "\n",
    "    employee_data = data_dict[employee]\n",
    "\n",
    "    if employee_data[\"poi\"]:\n",
    "        print employee"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 18 POIs (i.e. 'poi' = True) in our set of 146 total Enron employees. Note: this is much smaller than the 35 listed in the 'poi_names.txt' file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigation of feature 'from_poi_to_this_person'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n",
      "12\n",
      "74\n",
      "Proportion of non-'NaN' entries for 'from_poi_to_this_person': 0.58904109589\n"
     ]
    }
   ],
   "source": [
    "# find proportion of employees that had contact with POIs (i.e. 'from_poi_to_this_person' > 0)\n",
    "count_nan = 0\n",
    "count_no_contact = 0\n",
    "count_contact = 0\n",
    "\n",
    "for employee in data_dict:\n",
    "    for key in data_dict[employee]:\n",
    "        if key == 'from_poi_to_this_person':\n",
    "            if data_dict[employee][key] == 'NaN':\n",
    "                count_nan += 1\n",
    "            elif data_dict[employee][key] == 0:\n",
    "                count_no_contact += 1\n",
    "            elif data_dict[employee][key] > 0:\n",
    "                count_contact += 1\n",
    "            \n",
    "print count_nan\n",
    "print count_no_contact\n",
    "print count_contact\n",
    "\n",
    "print \"Proportion of non-'NaN' entries for 'from_poi_to_this_person':\", (float(count_contact) + count_no_contact) / len(data_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The no. of employees with direct email contact to a POI is 74, while those with no contact at all are only 12 employees. The proportion of employees for with we have data on this is 59%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identification of incorrect data entries\n",
    "\n",
    "Entries with the value 'NaN' can be handled appropriately by the classifier, however entries listed as 0.0 that should more accurately be 'NaN' will affect the model.\n",
    "\n",
    "There are only really 2 features for which an entry of 0.0 would be illogical for the given dataset: 'salary' and 'total_payments'. If either of these were truely 0.0 then this person would not be an Enron employee, and therefore may reasonably be removed from the dataset prior to modelling.\n",
    "\n",
    "If values of 0.0 are found in these features then they may also be transformed to 'NaN' so as to keep the other feature information for that employee."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of zero-entry salaries: 0\n",
      "Number of 'NaN'-entry salaries: 51\n",
      "Number of zero-entry total_payments: 0\n",
      "Number of 'NaN'-entry total_payments: 21\n"
     ]
    }
   ],
   "source": [
    "# search through 'salary' feature looking for entries listed as 0.0\n",
    "salary_zeros = []\n",
    "total_payment_zeros = []\n",
    "salary_nans = []\n",
    "total_payment_nans = []\n",
    "\n",
    "for employee in data_dict:\n",
    "    if data_dict[employee]['salary'] == 0:\n",
    "        salary_zeros.append(employee)\n",
    "    if data_dict[employee]['salary'] == 'NaN':\n",
    "        salary_nans.append(employee)\n",
    "    if data_dict[employee]['total_payments'] == 0:\n",
    "        total_payment_zeros.append(employee)\n",
    "    if data_dict[employee]['total_payments'] == 'NaN':\n",
    "        total_payment_nans.append(employee)\n",
    "\n",
    "print \"Number of zero-entry salaries:\", len(salary_zeros)\n",
    "print \"Number of 'NaN'-entry salaries:\", len(salary_nans)\n",
    "\n",
    "print \"Number of zero-entry total_payments:\", len(total_payment_zeros)\n",
    "print \"Number of 'NaN'-entry total_payments:\", len(total_payment_nans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like we don't have any zero-entry datapoints in features for which this would be a concern.\n",
    "\n",
    "There are however quite a few entries listed as 'NaN' for both 'salary' (51) and 'total_payments' (21). Although this information would be useful to have, the entries do not need to be formatted/processed further before applying the model so they will be left as 'NaN'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 'Manual' feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From intuition, I'd like to initially look at the following list of features:\n",
    "'poi', 'salary', 'exercised_stock_options', 'from_poi_to_this_person', 'from_this_person_to_poi', 'shared_receipt_with_poi'\n",
    "\n",
    "Also, would like to create a feature based on stock exercised but that is scaled to total_stock_value:\n",
    "i.e. \n",
    "\n",
    "scaled_stock_exercised = exercised_stock_options / total_stock_value\n",
    "\n",
    "I though this feature might be a good indicator of whether the employee understood the companies precarious position towards the end.\n",
    "\n",
    "The number of features selected (6) was chosen as I wanted the minimum number of features that represented the data. This could be reduced further by removing tightly-coupled features (for example: from_poi_to_this_person and from_this_person_to_poi)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting of chosen features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAERCAYAAAB4jRxOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nwIAywnCY40cAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x107000c50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pickle\n",
    "import sys\n",
    "sys.path.append(\"../tools/\")\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "features = [\"exercised_stock_options\", \"total_stock_value\"]\n",
    "data = featureFormat(data_dict, features)\n",
    "\n",
    "for point in data:\n",
    "    exercised = point[0]\n",
    "    total = point[1]\n",
    "    plt.scatter( exercised, total )\n",
    "\n",
    "plt.xlabel(\"exercised\")\n",
    "plt.ylabel(\"total\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There was only one MAJOR outlier observed in the financial features. It appears that this is the entry 'TOTAL'.\n",
    "\n",
    "As this is not an employee, I'll remove this data point from the dataset prior to analysis.\n",
    "\n",
    "(see Task 2: Outlier Removal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 'Automated' feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will use univariate feature selection tools from sklearn to automatically screen for the 'k-best' features that most powerfully describe the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(False, 'poi', 0.00016005424569618399), (False, 'salary', 0.23899588985313305), (False, 'deferral_payments', 0.34962715304280179), (True, 'total_payments', 2.5182610445203437), (False, 'loan_advances', 0.077948855777229875), (False, 'bonus', 0.0041731922805086684), (False, 'restricted_stock_deferred', 0.21950572394230994), (False, 'deferred_income', 0.16611912320976677), (False, 'total_stock_value', 0.01397841382175243), (False, 'expenses', 0.22826733729104948), (False, 'exercised_stock_options', 0.068194519159558625), (False, 'other', 0.022229270861607336), (False, 'long_term_incentive', 0.031333216297618476), (True, 'restricted_stock', 0.54908420147980874), (True, 'director_fees', 1.7516942790340737), (True, 'to_messages', 5.4466874833253529), (False, 'from_poi_to_this_person', 0.1587702392129193), (True, 'from_messages', 2.470521222656084), (True, 'from_this_person_to_poi', 8.9038215571655712)]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# list of ALL features\n",
    "# note: had to remove 'email_address' as it was the only str\n",
    "features_list = ['poi', 'salary',  'deferral_payments', 'total_payments', 'loan_advances', 'bonus', 'restricted_stock_deferred', 'deferred_income', 'total_stock_value', 'expenses', 'exercised_stock_options', 'other', 'long_term_incentive', 'restricted_stock', 'director_fees', 'to_messages', 'from_poi_to_this_person', 'from_messages', 'from_this_person_to_poi', 'shared_receipt_with_poi']\n",
    "\n",
    "### Extract features and labels from dataset for local testing\n",
    "data = featureFormat(data_dict, features_list, sort_keys = True)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "\n",
    "# perform feature selection (note: 'f_classif' used as the method to determine 'best' features)\n",
    "features_clf = SelectKBest(score_func=f_classif, k=6)\n",
    "features_clf = features_clf.fit(features, labels)\n",
    "\n",
    "# need to call get_support() to see which features were picked\n",
    "# created a tuple (using zip()) with the full feature_list to see which we the k-best features (in this case k=6)\n",
    "print zip(features_clf.get_support(), features_list, features_clf.scores_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the SelctKBest analysis, the 6 best features in the full feature set are: 'total_payments' (2.518), 'restricted_stock' (0.549), 'director_fees' (1.751), 'to_messages' (5.447), 'from_messages' (2.471), and 'from_this_person_to_poi' (8.9034).\n",
    "\n",
    "It appears that 'from_this_person_to_poi' is the strongest indicator.\n",
    "\n",
    "These are different from the intuitively-selected features (see above). This difference might have resulted from outliers, or skewed data leading to false trends picked up by the K-Best selector.\n",
    "\n",
    "I'll run the algorithm using both the 'manually' chosen feature set and this 'automatically' chosen set and compare the performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above data plotting, and also just looking at the list of employees in the dataset, two potential outliers were identified. These were:\n",
    "\n",
    "    'TOTAL'\n",
    "    and\n",
    "    'THE TRAVEL AGENCY IN THE PARK'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'salary': 26704229, 'to_messages': 'NaN', 'deferral_payments': 32083396, 'total_payments': 309886585, 'exercised_stock_options': 311764000, 'bonus': 97343619, 'restricted_stock': 130322299, 'shared_receipt_with_poi': 'NaN', 'restricted_stock_deferred': -7576788, 'total_stock_value': 434509511, 'expenses': 5235198, 'loan_advances': 83925000, 'from_messages': 'NaN', 'other': 42667589, 'from_this_person_to_poi': 'NaN', 'poi': False, 'director_fees': 1398517, 'deferred_income': -27992891, 'long_term_incentive': 48521928, 'email_address': 'NaN', 'from_poi_to_this_person': 'NaN'}\n"
     ]
    }
   ],
   "source": [
    "print data_dict['TOTAL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'salary': 'NaN', 'to_messages': 'NaN', 'deferral_payments': 'NaN', 'total_payments': 362096, 'exercised_stock_options': 'NaN', 'bonus': 'NaN', 'restricted_stock': 'NaN', 'shared_receipt_with_poi': 'NaN', 'restricted_stock_deferred': 'NaN', 'total_stock_value': 'NaN', 'expenses': 'NaN', 'loan_advances': 'NaN', 'from_messages': 'NaN', 'other': 362096, 'from_this_person_to_poi': 'NaN', 'poi': False, 'director_fees': 'NaN', 'deferred_income': 'NaN', 'long_term_incentive': 'NaN', 'email_address': 'NaN', 'from_poi_to_this_person': 'NaN'}\n"
     ]
    }
   ],
   "source": [
    "print data_dict['THE TRAVEL AGENCY IN THE PARK']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both of these entries were removed entirely from the dataset prior to analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: as the 'total' column might be useful for some types of analysis, instead of mutating\n",
    "# the origanal dict I will just create a modified copy of it.\n",
    "def removeEntry(d, key):\n",
    "    r = dict(d)\n",
    "    del r[key]\n",
    "    return r\n",
    "\n",
    "new_data_dict = removeEntry(data_dict, 'TOTAL')\n",
    "new_data_dict = removeEntry(new_data_dict, 'THE TRAVEL AGENCY IN THE PARK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144\n"
     ]
    }
   ],
   "source": [
    "print len(new_data_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coding up new features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scaled_stock_exercised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Write func to generically return the 'scaled' stock exercised\n",
    "def scaledStock(exercised, total):\n",
    "    \"\"\"\n",
    "    Compute the fraction of exercised stock options from the 'total stock' for a given employee.\n",
    "    Takes two features: the 'exercised_stock_options' and 'total_stock_value' and returns a single float.\n",
    "    \n",
    "    i.e. scaledStock(Int, Int) --> Float\n",
    "    \"\"\"\n",
    "    if (exercised == \"NaN\") or (total == \"NaN\"):\n",
    "        scaled_stock_exercised = \"NaN\"\n",
    "    elif exercised / total == 1:\n",
    "        scaled_stock_exercised = \"NaN\"\n",
    "    else:\n",
    "        scaled_stock_exercised = float(exercised) / total\n",
    "            \n",
    "    return scaled_stock_exercised\n",
    "\n",
    "\n",
    "### Perform scaling computation on Enron dataset and add as a new feature (i.e. a new\n",
    "### 'key' in the employee dict called 'scaled_stock_exercised')\n",
    "for employee in new_data_dict:\n",
    "\n",
    "    employee_data = new_data_dict[employee]\n",
    "    exercised_stock_options = employee_data[\"exercised_stock_options\"]\n",
    "    total_stock_value = employee_data[\"total_stock_value\"]\n",
    "    scaled_stock_exercised = scaledStock( exercised_stock_options, total_stock_value )\n",
    "    # print scaled_stock_exercised\n",
    "    employee_data[\"scaled_stock_exercised\"] = scaled_stock_exercised"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initial code for the scaledStock() function gave mostly reasonable values. However, there were a significant amount of entries of '1.0', which seems unlikely (i.e. that an employee sold ALL of the stock). There were also a few NEGATIVE values, which doesn't make much sense. Values of '0.0' are ok, as this was the default for if there was no entry for either 'exercised_stock_options' or 'total_stock_value' (i.e. 'NaN') but also might represent an employee who did not own any stock.\n",
    "\n",
    "Will try investigate the cases of scaled_stock_exercised = 1.0 more closely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, will find the negative entry!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BELFER ROBERT\n"
     ]
    }
   ],
   "source": [
    "for employee in new_data_dict:\n",
    "\n",
    "    employee_data = new_data_dict[employee]\n",
    "\n",
    "    if employee_data[\"scaled_stock_exercised\"] < 0:\n",
    "        print employee"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems to be just 'Robert Belfer'. If we print out his full entry we can see that his 'total_stock_value' is listed as -44093. This seems to be a data entry error, but unsure of the exact cause so will remove this employee from the dataset as the negative salary may affect other analyses. Also, given than most of his entries are 'NaN' this should not affect the dataset too much"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'to_messages': 'NaN', 'deferral_payments': -102500, 'expenses': 'NaN', 'poi': False, 'deferred_income': 'NaN', 'email_address': 'NaN', 'long_term_incentive': 'NaN', 'restricted_stock_deferred': 44093, 'shared_receipt_with_poi': 'NaN', 'loan_advances': 'NaN', 'from_messages': 'NaN', 'other': 'NaN', 'director_fees': 3285, 'bonus': 'NaN', 'total_stock_value': -44093, 'from_poi_to_this_person': 'NaN', 'scaled_stock_exercised': -0.07450162157258522, 'from_this_person_to_poi': 'NaN', 'restricted_stock': 'NaN', 'salary': 'NaN', 'total_payments': 102500, 'exercised_stock_options': 3285}\n"
     ]
    }
   ],
   "source": [
    "print data_dict['BELFER ROBERT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "### A COPY made of the dictionary with Robert Belfer's entry deleted to avoid mutating the original dataset\n",
    "def removeEmployee(d, key):\n",
    "    r = dict(d)\n",
    "    del r[key]\n",
    "    return r\n",
    "\n",
    "new_data_dict = removeEmployee(new_data_dict, 'BELFER ROBERT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now look at entries with scaled_stock_exercised = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for employee in new_data_dict:\n",
    "\n",
    "    employee_data = new_data_dict[employee]\n",
    "\n",
    "    if employee_data[\"scaled_stock_exercised\"] == 1.0:\n",
    "        # print new_data_dict[employee]\n",
    "        print employee, \":\", \"Exercised:\", new_data_dict[employee]['exercised_stock_options'], \"Total:\", new_data_dict[employee]['total_stock_value']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 24 employees whom exercised ALL of their stock. However, only one of these is listed as a POI ('HIRKO JOSEPH') and there is nothing overly suspicious of the others.\n",
    "\n",
    "They may simply have left Enron early and cashed in all of the stock, or needed to for some other reason.\n",
    "\n",
    "Solution: When performing the algorithm training, 'scaled_stock_exercised' values of 0.0 and 1.0 will be excluded to avoid any skewing effect, as these entries can be reasonably argued to be not informative in the search for fraud connections.\n",
    "\n",
    "This was acheived by re-writing the original feature-generating code so that values of 0.0 and 1.0 were set to \"NaN\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New feature check\n",
    "\n",
    "Just to see if the new feature ('scaled_stock_exercised') was actually of interest I printed out the names of the list of POI's for whom the percentage of their stock they sold off was >60%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HANNON KEVIN P\n",
      "RIEKER PAULA H\n",
      "SHELBY REX\n",
      "DELAINEY DAVID W\n",
      "LAY KENNETH L\n",
      "BELDEN TIMOTHY N\n",
      "RICE KENNETH D\n",
      "SKILLING JEFFREY K\n",
      "YEAGER F SCOTT\n"
     ]
    }
   ],
   "source": [
    "for employee in new_data_dict:\n",
    "\n",
    "    employee_data = new_data_dict[employee]\n",
    "\n",
    "    if employee_data[\"scaled_stock_exercised\"] >= 0.6 and employee_data[\"scaled_stock_exercised\"] != \"NaN\" \\\n",
    "    and employee_data[\"poi\"]:\n",
    "        print employee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for employee in new_data_dict:\n",
    "    employee_data = new_data_dict[employee]\n",
    "    # print employee_data[\"scaled_stock_exercised\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratio of POIs who sold >60% of their stock: 0.18\n",
      "Ratio in whole population: 0.125\n"
     ]
    }
   ],
   "source": [
    "count_sus = 0\n",
    "count_pop = 0\n",
    "for employee in new_data_dict:\n",
    "\n",
    "    employee_data = new_data_dict[employee]\n",
    "\n",
    "    if employee_data[\"scaled_stock_exercised\"] >= 0.6 and employee_data[\"scaled_stock_exercised\"] != \"NaN\" \\\n",
    "    and employee_data[\"poi\"]:\n",
    "        count_sus += 1\n",
    "    \n",
    "    if employee_data[\"scaled_stock_exercised\"] >= 0.6 and employee_data[\"scaled_stock_exercised\"] != \"NaN\":\n",
    "        count_pop += 1\n",
    "\n",
    "print \"Ratio of POIs who sold >60% of their stock:\", float(count_sus) / count_pop\n",
    "print \"Ratio in whole population:\", float(18) / 144"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ratio of POIs/non-POIs who sold off a lot of their stock isn't significantly different from that of the whole population (18% vs 12.5%). It is also a **very** small sample size, so will have a large vairance. \n",
    "\n",
    "The new 'scaled_stock_exercised' feature, therefore, may not be a strong indicator of POIs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choice of classifier algorithms to use include:\n",
    "\n",
    "    - Gaussian Naive-Bayes (GuassianNB)\n",
    "    - Decision Tree (DT)\n",
    "    - SVM\n",
    "    - K-Nearest Neighbors (KNN)\n",
    "    \n",
    "For each, will use SelectKBest to scan over entire feature-space (19 features). Will apply scaling where appropriate, but no other parameter tuning will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Store to my_dataset for easy export below.\n",
    "# my_dataset = data_dict \t# use updated dict instead\n",
    "my_dataset = new_data_dict\n",
    "\n",
    "features_list = ['poi',\n",
    "                 'salary', \n",
    "                 'scaled_stock_exercised', \n",
    "                 'from_poi_to_this_person', \n",
    "                 'from_this_person_to_poi', \n",
    "                 'shared_receipt_with_poi', \n",
    "                 'to_messages', \n",
    "                 'from_messages',\n",
    "                 'deferral_payments', \n",
    "                 'total_payments', \n",
    "                 'bonus', \n",
    "                 'restricted_stock_deferred', \n",
    "                 'long_term_incentive', \n",
    "                 'loan_advances', \n",
    "                 'deferred_income', \n",
    "                 'total_stock_value', \n",
    "                 'expenses', \n",
    "                 'exercised_stock_options',\n",
    "                 'director_fees', \n",
    "                 'restricted_stock'\n",
    "                ]\n",
    "\n",
    "### Extract features and labels from dataset for local testing\n",
    "data = featureFormat(my_dataset, features_list, sort_keys = True)\n",
    "labels, features = targetFeatureSplit(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import relevant modules\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from time import time\n",
    "\n",
    "from tester import test_classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian NB model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/TBD/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/TBD/anaconda/lib/python2.7/site-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [12] are constant.\n",
      "  UserWarning)\n",
      "/Users/TBD/anaconda/lib/python2.7/site-packages/sklearn/feature_selection/univariate_selection.py:114: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('scaling', MinMaxScaler(copy=True, feature_range=(0, 1))), ('kbest', SelectKBest(k=14, score_func=<function f_classif at 0x113724a28>)), ('gnb', GaussianNB(priors=None))])\n",
      "\tAccuracy: 0.77460\tPrecision: 0.26991\tRecall: 0.40500\tF1: 0.32394\tF2: 0.36815\n",
      "\tTotal predictions: 15000\tTrue positives:  810\tFalse positives: 2191\tFalse negatives: 1190\tTrue negatives: 10809\n",
      "\n",
      "Time taken: 141.937551022\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "t0 = time()\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "kbest = SelectKBest()\n",
    "cv = StratifiedShuffleSplit(labels,1000,random_state=42)\n",
    "clf_nb = GaussianNB()\n",
    "pipe = Pipeline([('scaling',scaler),('kbest',kbest),('gnb',clf_nb)])\n",
    "parameters = {'kbest__k':range(1,19)}\n",
    "\n",
    "gs = GridSearchCV(pipe,param_grid=parameters,cv=cv, scoring='f1')\n",
    "gs.fit(features,labels)\n",
    "clf = gs.best_estimator_\n",
    "test_classifier(clf, new_data_dict, features_list, folds=1000)\n",
    "\n",
    "print \"Time taken:\", (time() - t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GaussianNB results:\n",
    "\n",
    "    Accuracy: 0.77460\n",
    "    Precision: 0.26991\n",
    "    Recall: 0.40500\n",
    "    F1: 0.32394\n",
    "    k = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('kbest', SelectKBest(k=3, score_func=<function f_classif at 0x113724a28>)), ('dt', DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best'))])\n",
      "\tAccuracy: 0.81440\tPrecision: 0.32406\tRecall: 0.36100\tF1: 0.34153\tF2: 0.35295\n",
      "\tTotal predictions: 15000\tTrue positives:  722\tFalse positives: 1506\tFalse negatives: 1278\tTrue negatives: 11494\n",
      "\n",
      "Time taken: 147.122808933\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "t0 = time()\n",
    "\n",
    "kbest = SelectKBest()\n",
    "cv = StratifiedShuffleSplit(labels,1000,random_state=42)\n",
    "clf_dt = DecisionTreeClassifier()\n",
    "pipe = Pipeline([('kbest',kbest),('dt',clf_dt)])\n",
    "parameters = {'kbest__k':range(1,19)}\n",
    "\n",
    "gs = GridSearchCV(pipe,param_grid=parameters,cv=cv, scoring='f1')\n",
    "gs.fit(features,labels)\n",
    "clf = gs.best_estimator_\n",
    "test_classifier(clf, new_data_dict, features_list, folds=1000)\n",
    "\n",
    "print \"Time taken:\", (time() - t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Results:\n",
    "\n",
    "    Accuracy: 0.81440\n",
    "    Precision: 0.32406\n",
    "    Recall: 0.36100\n",
    "    F1: 0.34153\n",
    "    k = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('scaling', MinMaxScaler(copy=True, feature_range=(0, 1))), ('kbest', SelectKBest(k=1, score_func=<function f_classif at 0x113724a28>)), ('svc', SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False))])\n",
      "\tAccuracy: 0.86667\tPrecision: 0.50000\tRecall: 0.04150\tF1: 0.07664\tF2: 0.05082\n",
      "\tTotal predictions: 15000\tTrue positives:   83\tFalse positives:   83\tFalse negatives: 1917\tTrue negatives: 12917\n",
      "\n",
      "Time taken: 181.322751999\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "t0 = time()\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "kbest = SelectKBest()\n",
    "cv = StratifiedShuffleSplit(labels,1000,random_state=42)\n",
    "clf_svm = SVC()\n",
    "pipe = Pipeline([('scaling',scaler),('kbest',kbest),('svc',clf_svm)])\n",
    "parameters = {'kbest__k':range(1,19)}\n",
    "\n",
    "gs = GridSearchCV(pipe,param_grid=parameters,cv=cv, scoring='f1')\n",
    "gs.fit(features,labels)\n",
    "clf = gs.best_estimator_\n",
    "test_classifier(clf, new_data_dict, features_list, folds=1000)\n",
    "\n",
    "print \"Time taken:\", (time() - t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Results\n",
    "\n",
    "    Accuracy: 0.86667\n",
    "    Precision: 0.50000\n",
    "    Recall: 0.04150\n",
    "    F1: 0.07664\n",
    "    k = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNearestNeighbors Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('scaling', MinMaxScaler(copy=True, feature_range=(0, 1))), ('kbest', SelectKBest(k=3, score_func=<function f_classif at 0x113724a28>)), ('knn', KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
      "           weights='uniform'))])\n",
      "\tAccuracy: 0.87840\tPrecision: 0.64058\tRecall: 0.20050\tF1: 0.30541\tF2: 0.23244\n",
      "\tTotal predictions: 15000\tTrue positives:  401\tFalse positives:  225\tFalse negatives: 1599\tTrue negatives: 12775\n",
      "\n",
      "Time taken: 215.164536953\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "t0 = time()\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "kbest = SelectKBest()\n",
    "cv = StratifiedShuffleSplit(labels,1000,random_state=42)\n",
    "clf_knn = KNeighborsClassifier()\n",
    "pipe = Pipeline([('scaling',scaler),('kbest',kbest),('knn',clf_knn)])\n",
    "parameters = {'kbest__k':range(1,19)}\n",
    "\n",
    "gs = GridSearchCV(pipe,param_grid=parameters,cv=cv, scoring='f1')\n",
    "gs.fit(features,labels)\n",
    "clf = gs.best_estimator_\n",
    "test_classifier(clf, new_data_dict, features_list, folds=1000)\n",
    "\n",
    "print \"Time taken:\", (time() - t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN Results:\n",
    "\n",
    "    Accuracy: 0.87840\n",
    "    Precision: 0.64058\n",
    "    Recall: 0.20050\n",
    "    F1: 0.30541\n",
    "    k = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choice of algorithm to use\n",
    "\n",
    "From the initial results, the algorithms attempted had quite different precision/recall scores.\n",
    "\n",
    "As the Decision Tree classifier was the only one to return precision & recall scores BOTH above 0.3 (set as the minimum requirement), this algorithm was selected to use for further paramter tuning in attempts to get the maximum performance from the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter tuning with GridSearchCV\n",
    "\n",
    "A pipeline() operator was used to chain feature-scaling, principle component analysis (PCA), and automatic feature selection (selectKBest). Also, the features list was expanded to include ALL email AND financial features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify SelectKBest 'scores' for features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('expenses', 24.532874853652139),\n",
      " ('deferred_income', 23.896807476321385),\n",
      " ('total_payments', 20.524645181851792),\n",
      " ('poi', 18.003739993113935),\n",
      " ('loan_advances', 11.321486775141238),\n",
      " ('restricted_stock_deferred', 9.7721035384082544),\n",
      " ('director_fees', 9.0790766616708698),\n",
      " ('deferral_payments', 8.6737591099353661),\n",
      " ('from_this_person_to_poi', 8.4326354230246814),\n",
      " ('long_term_incentive', 7.1253824688830685),\n",
      " ('total_stock_value', 5.9545442921972933),\n",
      " ('scaled_stock_exercised', 5.1422191945069704),\n",
      " ('from_poi_to_this_person', 2.3388361146462624),\n",
      " ('exercised_stock_options', 2.1345269671629525),\n",
      " ('shared_receipt_with_poi', 1.5942560277180795),\n",
      " ('salary', 1.34526701420679),\n",
      " ('from_messages', 0.23671093690118508),\n",
      " ('to_messages', 0.1753832041587958),\n",
      " ('bonus', 0.065472170123396331)]\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "import pprint as pp\n",
    "\n",
    "select = SelectKBest()\n",
    "select.fit(features, labels)\n",
    "zipped = zip(features_list, select.scores_)\n",
    "\n",
    "print pp.pprint(sorted(zipped, key = lambda x: x[1], reverse=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The highest scoring feature is 'expenses', while other financial features also make up the top 7 (excluding 'poi').\n",
    "\n",
    "The highest scoring email-based feature is 'from_this_person_to_poi', which matches intuition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify 'explained_variance_ratio' from PCA on features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('poi', 0.8065149744773602),\n",
      " ('salary', 0.1481230856144698),\n",
      " ('scaled_stock_exercised', 0.016496669482876172),\n",
      " ('from_poi_to_this_person', 0.014488479201479075),\n",
      " ('from_this_person_to_poi', 0.0075127834727692),\n",
      " ('shared_receipt_with_poi', 0.0033949422753844884),\n",
      " ('to_messages', 0.0019073268723289669),\n",
      " ('from_messages', 0.0012559359089426641),\n",
      " ('deferral_payments', 0.00019023774587439093),\n",
      " ('total_payments', 6.1020782403630892e-05),\n",
      " ('bonus', 4.2042104347105577e-05),\n",
      " ('restricted_stock_deferred', 8.5938766810307884e-06),\n",
      " ('long_term_incentive', 3.8782102433948897e-06),\n",
      " ('loan_advances', 2.2303105575575284e-08),\n",
      " ('deferred_income', 7.0189484958619057e-09),\n",
      " ('total_stock_value', 6.2850038335915171e-10),\n",
      " ('expenses', 1.4011464763376828e-11),\n",
      " ('exercised_stock_options', 1.0273349110297789e-11),\n",
      " ('director_fees', 4.8810466936574354e-16)]\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import pprint as pp\n",
    "\n",
    "pca = PCA()\n",
    "pca.fit(features)\n",
    "zipped = zip(features_list, pca.explained_variance_ratio_)\n",
    "print pp.pprint(sorted(zipped, key = lambda x: x[1], reverse=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Appears that 'salary' is by far the largest explainer of variance in the dataset.\n",
    "\n",
    "The created feature 'scaled_stock_exercised', and also 'from_poi_to_this_person'/'from_this_person_to_poi' also rank quite highly, which matches well with intuition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score is 0.432476190476\n",
      "Best params: {'model__max_features': 'sqrt', 'model__splitter': 'random', 'model__class_weight': 'balanced', 'model__criterion': 'gini', 'kbest__k': 16, 'model__min_samples_split': 4, 'model__random_state': 42}\n",
      "Time taken: 666.469586849\n"
     ]
    }
   ],
   "source": [
    "##==========  TUNING THE DECISION TREE ================##\n",
    "### find the best parameter setting using GridSearchCV\n",
    "\n",
    "clf = DecisionTreeClassifier(max_depth=None)\n",
    "\n",
    "# start timer\n",
    "t0 = time()\n",
    "\n",
    "# split the dataset randomly for cross-validation\n",
    "cv = StratifiedShuffleSplit(labels, 50, random_state = 42)      # set to 1000 for final test\n",
    "\n",
    "dt_params = {\n",
    "    'model__criterion' : ('gini', 'entropy'), \n",
    "    'model__splitter' : ('best', 'random'), \n",
    "    'model__min_samples_split' : [4, 8, 16],\n",
    "    'model__max_features' : (None, 'sqrt', 'log2'),\n",
    "    #'model__max_depth' : [None, 10, 100],  # None found to be the best\n",
    "    'model__random_state' : [42],\n",
    "    'model__class_weight' : ['balanced', None],\n",
    "    'kbest__k' : range(1, 19)\n",
    "    }\n",
    "\n",
    "### INSERT PIPELINE HERE TO PERFORM SCALING + PCA? + GRIDSEARCH\n",
    "pipe = Pipeline([\n",
    "    #('scaler', MinMaxScaler()), # note necessary for tree-based algorithm\n",
    "    #('pca', PCA()),   # also suggested not necessary for trees\n",
    "    ('kbest', SelectKBest()),   # k-best here selects the k best COMPONENTS (from the PCA analysis)    \n",
    "    ('model', clf)\n",
    "    ])\n",
    "\n",
    "dt_search = GridSearchCV(pipe, param_grid=dt_params, cv=cv, scoring='f1')\n",
    "dt_search.fit(features, labels)\n",
    "\n",
    "print \"Best score is {0}\".format(dt_search.best_score_)\n",
    "print \"Best params:\", dt_search.best_params_\n",
    "\n",
    "print \"Time taken:\", (time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Assign the optimized estimator to the DT clf obj\n",
    "dt_clf = dt_search.best_estimator_\n",
    "\n",
    "# set optimized DT clf to 'clf' to overwrite unoptimized one\n",
    "clf = dt_clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For 50 folds the above gave:\n",
    "precision = 0.39695\n",
    "recall = 0.52000\n",
    "\n",
    "Changing 'max_depth' to 10 gave: prcision = 0.37 recall = 0.60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding features selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('exercised_stock_options', 17.670314794600113),\n",
      " ('poi', 16.24941209142386),\n",
      " ('long_term_incentive', 12.608069399977163),\n",
      " ('restricted_stock_deferred', 8.1039631243259809),\n",
      " ('deferred_income', 7.2827060742576348),\n",
      " ('total_stock_value', 7.2274462211359394),\n",
      " ('director_fees', 5.6778173146794542),\n",
      " ('shared_receipt_with_poi', 4.2070436676607477),\n",
      " ('from_poi_to_this_person', 3.8508113006202156),\n",
      " ('expenses', 1.9695419354726496),\n",
      " ('from_this_person_to_poi', 1.8093923809926191),\n",
      " ('total_payments', 1.2923265504983037),\n",
      " ('from_messages', 0.43430878669400097),\n",
      " ('deferral_payments', 0.2007242740666996),\n",
      " ('scaled_stock_exercised', 0.1710181222715203),\n",
      " ('loan_advances', 0.065334723579955734)]\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pprint as pp\n",
    "\n",
    "# which are the 16 'best' features that GridSearchCV has selected\n",
    "feature_indices = dt_search.best_estimator_.named_steps[\"kbest\"].get_support(indices=True)\n",
    "selected_features_list = [features_list[i] for i in feature_indices]\n",
    "\n",
    "# get the scores for the selected features\n",
    "selected_feature_scores = dt_search.best_estimator_.named_steps[\"kbest\"].scores_\n",
    "\n",
    "# zip the two lists\n",
    "zipped = zip(selected_features_list, selected_feature_scores)\n",
    "print pp.pprint(sorted(zipped, key = lambda x: x[1], reverse=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calling generated classifier on 'tester.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('kbest', SelectKBest(k=16, score_func=<function f_classif at 0x113724a28>)), ('model', DecisionTreeClassifier(class_weight='balanced', criterion='gini',\n",
      "            max_depth=None, max_features='sqrt', max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=4, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=42, splitter='random'))])\n",
      "\tAccuracy: 0.80593\tPrecision: 0.33382\tRecall: 0.45750\tF1: 0.38599\tF2: 0.42594\n",
      "\tTotal predictions: 15000\tTrue positives:  915\tFalse positives: 1826\tFalse negatives: 1085\tTrue negatives: 11174\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tester import test_classifier\n",
    "\n",
    "test_classifier(clf, new_data_dict, features_list, folds = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "The tuned Decision Tree Classifier appears to pass! \n",
    "\n",
    "When using my created feature ('scaled_stock_exercised') the metrics were:\n",
    "\n",
    "    Accuracy: 0.80593\n",
    "    Precision: 0.33382\n",
    "    Recall: 0.45750\n",
    "\n",
    "Ommiting the created feature from the 'features_list' gave:\n",
    "\n",
    "    Accuracy: 0.78867\n",
    "    Precision: 0.34358\n",
    "    Recall: 0.64250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
