---
title: "Leonard Cohen Lyrical Analysis"
output: html_notebook
---

# Analysis of Leonard Cohen lyrics
Having collected a corpus of lyrics for all of Leonard Cohen's back-catalogue, some exploratory analysis will be performed. The aim is to eventually use sentiment analysis and other NLP approaches to get a sense of how Leonard's outlook and approach to songwriting changed throughout his career.

This analysis follows a similar investigation conducted on Prince's lyrics (see: https://www.datacamp.com/community/tutorials/sentiment-analysis-R?utm_campaign=News&utm_medium=Community&utm_source=DataCamp.com)

### Loading of required libraries

```{r}
library(dplyr) #Data manipulation (also included in the tidyverse package)
library(tidytext) #Text mining
library(tidyr) #Spread, separate, unite, text mining (also included in the tidyverse package)
library(widyr) #Use for pairwise correlation

#Visualizations!
library(ggplot2) #Visualizations (also included in the tidyverse package)
library(ggrepel) #`geom_label_repel`
library(gridExtra) #`grid.arrange()` for multi-graphs
library(knitr) #Create nicely formatted output tables
library(kableExtra) #Create nicely formatted output tables
library(formattable) #For the color_tile function
library(circlize) #Visualizations - chord diagram
#library(memery) #Memes - images with plots
library(magick) #Memes - images with plots (image_read)
library(yarrr)  #Pirate plot
library(radarchart) #Visualizations
library(igraph) #ngram network diagrams
library(ggraph) #ngram network diagrams

#Define some colors to use throughout
my_colors <- c("#E69F00", "#56B4E9", "#009E73", "#CC79A7", "#D55E00", "#D65E00")

#Customize ggplot2's default theme settings
#This tutorial doesn't actually pass any parameters, but you may use it again in future tutorials so it's nice to have the options
theme_lyrics <- function(aticks = element_blank(),
                         pgminor = element_blank(),
                         lt = element_blank(),
                         lp = "none")
{
  theme(plot.title = element_text(hjust = 0.5), #Center the title
        axis.ticks = aticks, #Set axis ticks to on or off
        panel.grid.minor = pgminor, #Turn the minor grid lines on or off
        legend.title = lt, #Turn the legend title on or off
        legend.position = lp) #Turn the legend on or off
}

#Customize the text tables for consistency using HTML formatting
my_kable_styling <- function(dat, caption) {
  kable(dat, "html", escape = FALSE, caption = caption) %>%
  kable_styling(bootstrap_options = c("striped", "condensed", "bordered"),
                full_width = FALSE)
}
```

### Text/lyrics pre-processing

Have used a python script to pre-process some of the song text files and save in csv format.

Will load here into R for further manipulation.

```{r}
leonard_cohen <- read.csv("leonard_songs.csv", stringsAsFactors = FALSE)
```

See what columns we have using the names() function.

```{r}
names(leonard_cohen)
```

Where "X"" is just an index column, and "peak" is the peak charting position (in any country) for a given song.

Tidy this up and rename the text column.
```{r}
cohen <- leonard_cohen %>%
  select(lyrics = text, song, year, album, peak)

# take a 'glimpse' at the 39th entry
glimpse(cohen[39,])
```

How many songs are there in this dataset?
```{r}
dim(cohen)
```

Print the lyrics for one of the songs to see how they are structured.
```{r}
str(cohen[39,]$lyrics, nchar.max = 300)
```

Lots of newline characters that will need to be removed.
Also, we want to expand out any contractions. Can use the gsub() method (replaces string sub-sections) and apply to all lyrics data.
```{r}
# function to expand contractions in an English-language source
fix.contractions <- function(doc) {
  # "won't" is a special case as it does not expand to "wo not"
  doc <- gsub("won't", "will not", doc)
  doc <- gsub("can't", "can not", doc)
  doc <- gsub("n't", " not", doc)
  doc <- gsub("'ll", " will", doc)
  doc <- gsub("'re", " are", doc)
  doc <- gsub("'ve", " have", doc)
  doc <- gsub("'m", " am", doc)
  doc <- gsub("'d", " would", doc)
  # 's could be 'is' or could be possessive: it has no expansion
  doc <- gsub("'s", "", doc)
  return(doc)
}

# fix (expand) contractions by applying the above function across all lyrics
cohen$lyrics <- sapply(cohen$lyrics, fix.contractions)
```

```{r}
str(cohen[39,]$lyrics, nchar.max = 300)
```

Great, that seemed to work well.
Now we can remove any special characters using a regex and lowercase everything.
```{r}
# function to remove special characters (note that '/n' is a special character)
removeSpecialChars <- function(x) gsub("[^a-zA-Z0-9 ]", " ", x)
# remove special characters
cohen$lyrics <- sapply(cohen$lyrics, removeSpecialChars)
# convert everything to lower case
cohen$lyrics <- sapply(cohen$lyrics, tolower)
```
We won't do word stemming at this stage.
Let's have a look at the summary now.
```{r}
#get facts about the full dataset
summary(cohen)
```
### Create a few new fields
We can add a few fields/columns based on the existing data.
```{r}
#create the decade column
cohen <- cohen %>%
  mutate(decade = 
           ifelse(cohen$year %in% 1967:1969, "1960s",
           ifelse(cohen$year %in% 1970:1979, "1970s",
           ifelse(cohen$year %in% 1980:1989, "1980s", 
           ifelse(cohen$year %in% 1990:1999, "1990s", 
           ifelse(cohen$year %in% 2000:2009, "2000s", 
           ifelse(cohen$year %in% 2010:2019, "2010s", 
                  "NA")))))))
```
Create a column binning the chart position as well.
```{r}
#create the chart level column
cohen <- cohen %>%
  mutate(chart_level = 
           ifelse(cohen$peak %in% 1:10, "Top 10", 
           ifelse(cohen$peak %in% 11:100, "Top 100", "Uncharted")))
```
Create a binary column indicating whether a song charted (Top 100) or not. Then export this data as a csv for future use.
```{r}
#create binary field called charted showing if a song hit the charts at all
cohen <- cohen %>%
  mutate(charted = 
           ifelse(cohen$peak %in% 1:100, "Charted", "Uncharted"))

#save the new dataset to .csv for use in later tutorials
write.csv(cohen, file = "cohen_new.csv")
```

## Descriptive Statistics
Will first define a custom colour scheme so that all plots will be styled consistently.

```{r}
#define some colors to use throughout
my_colors <- c("#E69F00", "#56B4E9", "#009E73", "#CC79A7", "#D55E00")

theme_lyrics <- function() 
{
  theme(plot.title = element_text(hjust = 0.5),
        axis.text.x = element_blank(), 
        axis.ticks = element_blank(),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        legend.position = "none")
}
```

Using dplyr's group_by() and summarise() functions let's look at how many songs Leonard Cohen released by decade. The n() function is a useful aggregate func to use with summarise on grouped data. Then use ggplot() and geom_bar() to create a bar chart of the results, and fill by the the 'charted' category.
```{r}
cohen %>%
  group_by(decade, charted) %>%
  summarise(number_of_songs = n()) %>%
  ggplot() + 
  geom_bar(aes(x = decade, y = number_of_songs, 
               fill = charted), stat = "identity")  +
  theme(plot.title = element_text(hjust = 0.5),
        legend.title = element_blank(),
        panel.grid.minor = element_blank()) +
  ggtitle("Released Songs") +
  labs(x = NULL, y = "Song Count")
```
This shows a pretty unusual career -- he took the 1990s off to become a monk (true story)! Other wise the output was pretty consistent for his whole life.

Now create a similar chart with only charting songs, and fill by 'chart level'.
Will filter out any non charting songs. Then pipe the output grouped data directly into the ggplot() and geom_bar() funcs.
```{r}
charted_songs_over_time <- cohen %>%
  filter(peak > 0 & peak < 100) %>%
  group_by(decade, chart_level) %>%
  summarise(number_of_songs = n())

charted_songs_over_time %>% 
  ggplot() + 
  geom_bar(aes(x = decade, y = number_of_songs, 
               fill = chart_level), stat = "identity") +
  theme(plot.title = element_text(hjust = 0.5),
        legend.title = element_blank(),
        panel.grid.minor = element_blank()) +
  labs(x = NULL, y = "Song Count") +
  ggtitle("Charted Songs")
```
Note that the total count is quite low (max = 5), but it seems the Leonard's most successful charting decade was easily the 1980s.

### Let's look at the Top 10 charting songs only
Note that we'll use kable() and kable_styling() from the 'knitr' and 'kableExtra' packages and color_tile() from 'formattable' to create nicely formatted HTML output!
```{r}
library(knitr) # for dynamic reporting
library(kableExtra) # create a nicely formated HTML table
library(formattable) # for the color_tile function
cohen %>%
  filter(chart_level == "Top 10") %>%
  select(year, song, peak) %>%
  arrange(year) %>%
  mutate(year = color_tile("lightblue", "lightgreen")(year)) %>%
  mutate(peak = color_tile("lightgreen", "lightgreen")(peak)) %>%
  kable("html", escape = FALSE, align = "c", caption = "Cohen Songs That Made The Top 10") %>%
  kable_styling(bootstrap_options = 
                  c("striped", "condensed", "bordered"), 
                  full_width = TRUE)
```

## Text Mining
'Lexical complexity' can mean different things in different contexts, but simplistically we can assume that it's described by a combination of the following:
  * Word Frequency: number of words per song
  * Word Length: average length of individual words in a text
  * Lexical Diversity: number of unique words used in a text (song vocabulary)
  * Lexical Density: the number of unique words divided by the total number of words (word repetition)

### Text tokenization
To start, we need to split the lyrics into individual words - a process known as 'tokenizing'.

For this analysis, we'll use a data format called 'Tidy Text'. This uses the 'tidytext' package to do the tokenization, where the output will be a table with one token (word or n-gram) per row.

Before we do the tokenization step, we can remove any undesirable words from the lyrics data that may negatively affect the analysis (for example, his name).
```{r}
undesirable_words <- c("leonard")
```

Using the 'tidytext' package we will break the text into individual words (i.e. tokenize), and then transform it into a tidy data structure. To do this, we use tidytext's unnest_tokens() function. This requires at least two arguments: the output column name that will be created as the text is unnested ("word" in this case), and the input column containing the text to be tokenized (i.e. lyrics).

We'll first pipe the 'cohen' df into the unnest_tokens() func, then remove stopwords using tidytext's built-in 'stop_words' feature.

After tokenizing, use dplyr's anti_join() to remove the stop words, then filter out the 'undersirable_words' defined above. Then, use distinct() to get rid of any duplicate records, and lastly, remove any words that have fewer than four characters - often these are interjections such as "hey" and "yea".
Store this new dataset in a df called 'cohen_words_filtered'.
```{r}
#unnest and remove stop, undesirable and short words
cohen_words_filtered <- cohen %>%
  unnest_tokens(word, lyrics) %>%
  anti_join(stop_words) %>%
  distinct() %>%
  filter(!word %in% undesirable_words) %>%
  filter(nchar(word) > 3)
```
Note: The 'stop_words' found in tidytext has a 'word' column, and therefore as we created a 'word' column from our text using 'anti_join()' in this way effectively filters out the stop words.

Look at the 'class' and dimensions of the new df.
```{r}
class(cohen_words_filtered)
```
```{r}
dim(cohen_words_filtered)
```
We can see it is a dataframe with 8 fields and 5737 observations (rows).

Let's pick out one word an inspect its appearance in the lyrics corpus, limiting it to 10 instances.
```{r}
cohen_words_filtered %>% 
  filter(word == "dance") %>%
  select(word, song, year, peak, decade, chart_level, charted) %>%
  arrange() %>%
  #top_n(5, song) %>%
  mutate(song = color_tile("lightblue","lightblue")(song)) %>%
  mutate(word = color_tile("lightgreen","lightgreen")(word)) %>%
  kable("html", escape = FALSE, align = "c", caption = "Tokenized Format Example") %>%
  kable_styling(bootstrap_options = 
                  c("striped", "condensed", "bordered"), 
                  full_width = TRUE)
```

### Word Frequency
Let's get a summarized count of words per song. We'll need to unnest the original lyrics dataset (so that it includes stop words).
```{r}
full_word_count <- cohen %>%
  unnest_tokens(word, lyrics) %>%
  group_by(song, chart_level) %>%
  summarise(num_words = n()) %>%
  arrange(desc(num_words)) 

full_word_count[1:10,] %>%
  ungroup(num_words, song) %>%
  mutate(num_words = color_bar("lightblue")(num_words)) %>%
  mutate(song = color_tile("lightpink","lightpink")(song)) %>%
  kable("html", escape = FALSE, align = "c", caption = "Songs With Highest Word Count") %>%
  kable_styling(bootstrap_options = 
                  c("striped", "condensed", "bordered"), 
                  full_width = TRUE)
```
These appear as we would expect - Master Song, Dress Rehearsal Rag, and The Stranger Song are all rambling, wordy ballads!

Create a histogram to get a sense of how many words Leonard typically uses for a song.
```{r}
full_word_count %>%
  ggplot() +
    geom_histogram(aes(x = num_words, fill = chart_level )) +
    ylab("Song Count") + 
    xlab("Word Count per Song") +
    ggtitle("Word Count Distribution") +
    theme(plot.title = element_text(hjust = 0.5),
          legend.title = element_blank(),
          panel.grid.minor.y = element_blank())
```
Around about 200 words appears the most common range for a Cohen song.

### Top Words
To do a simple evaluation of the most frequently used words, we can use count() and top_n() to get the 'n' top words from the clean, filtered dataset. Then reorder() to sort words according to the count and use dplyr's mutate() verb to reassign the ordered value to 'word'. This allows ggplot() to display it nicely.
```{r}
cohen_words_filtered %>%
  count(word, sort = TRUE) %>%
  top_n(10) %>%
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot() +
    geom_col(aes(word, n), fill = my_colors[4]) +
    theme(legend.position = "none", 
          plot.title = element_text(hjust = 0.5),
          panel.grid.major = element_blank()) +
    xlab("") + 
    ylab("Song Count") +
    ggtitle("Most Frequently Used Words in Leonard Cohen Lyrics") +
    coord_flip()
```
Pretty unsuprising that 'love' is by far the most used word! Also, 'heart', 'woman', 'baby', and 'body' all indicate what was typically on Leonard's mind...

Did Leonard change his lyrical themes over the course of his career? Let's create a similar plot, but faceted by decade.
(Note: we use ggplot's facet_wrap() to plot multiple bar charts beside each other)
```{r}
timeless_words <- cohen_words_filtered %>% 
  group_by(decade) %>%
  count(word, decade, sort = TRUE) %>%
  slice(seq_len(8)) %>%
  ungroup() %>%
  arrange(decade,n) %>%
  mutate(row = row_number()) 

timeless_words %>%
  ggplot(aes(row, n, fill = decade)) +
    geom_col(show.legend = NULL) +
    labs(x = NULL, y = "Song Count") +
    ggtitle("Timeless Words") + 
    theme_lyrics() +  
    facet_wrap(~decade, scales = "free", ncol = 5) +
    scale_x_continuous(  # This handles replacement of row 
      breaks = timeless_words$row, # notice need to reuse data frame
      labels = timeless_words$word) +
    coord_flip()
```
Turns out 'love' endures! Nice :)

### Lexical Diversity
What about how many UNIQUE words are in a typical Cohen song? This is pretty straightforward to analyze.

We'll tokenize the original lyrics dataset again so as to include all the stop words and small words in the count().
```{r}
lex_diversity_per_year <- cohen %>%
  unnest_tokens(word, lyrics) %>%
  group_by(song,year) %>%
  summarise(lex_diversity = n_distinct(word)) %>%
  arrange(desc(lex_diversity)) 

diversity_plot <- lex_diversity_per_year %>%
  ggplot(aes(year, lex_diversity)) +
    geom_point(color = my_colors[3],
               alpha = .4, 
               size = 4, 
               position = "jitter") + 
    stat_smooth(color = "black", se = FALSE, method = "lm") +
    geom_smooth(aes(x = year, y = lex_diversity), se = FALSE,
                color = "blue", lwd = 2) +
    ggtitle("Lexical Diversity") +
    xlab("Year") + 
    ylab("No. of unique words") +
    scale_color_manual(values = my_colors) +
    theme_classic() + 
    theme_lyrics()

diversity_plot
```
It seems Leonard has been simplifying his vocabulary over the years. This makes sense, as his songs have become somewhat more concise and direct compared with his early more story-telling singer-songwriter style.

### Lexical Density
Remember from above that 'lexical density' refers to the number of unique words divided by the total number of words (per song). This relates to word repetition - a technique commonly employed in songwriting; i.e. more lyrical repetition in a song would decrease its lexical density.
```{r}
lex_density_per_year <- cohen %>%
  unnest_tokens(word, lyrics) %>%
  group_by(song,year) %>%
  summarise(lex_density = n_distinct(word)/n()) %>% # calculate lexical density
  arrange(desc(lex_density))

density_plot <- lex_density_per_year %>%
  ggplot(aes(year, lex_density)) + 
    geom_point(color = my_colors[4],
               alpha = .4, 
               size = 4, 
               position = "jitter") + 
    stat_smooth(color = "black", 
                se = FALSE, 
                method = "lm") +
    geom_smooth(aes(x = year, y = lex_density), 
                se = FALSE,
                color = "blue", 
                lwd = 2) +
    ggtitle("Lexical Density") + 
    xlab("Year") + 
    ylab("Density") +
    scale_color_manual(values = my_colors) +
    theme_classic() + 
    theme_lyrics()

density_plot
```
This again indicates that Leonard adopted a more simplistic songwriting approach in his later years, although the difference is not very much.

## Using TF-IDF
Term Frequency-Inverse Document Frequency (TF-IDF) is a method of trying to figure out what the most IMPORTANT words are in a text corpus, i.e. not simply the most common.

What do we see when we use TF-IDF on Cohen's corpus to identify the most distinctive words that he uses in his songs.

The 'tidytext' packages provides an implementation of TF-IDF with the bind_tf_idf() function. It takes a tidy text dataset as input with one row per token (word), per document (song).

Again using the original lyrics dataset, we'll remove the 'undesirable_words' but leave in stop words (as if they are common they won't be assigned much significance anyway). However, we'll still remove small words (<3 chars) just to reduce the vocab size.
```{r}
popular_tfidf_words <- cohen %>%
  unnest_tokens(word, lyrics) %>%
  distinct() %>%
  filter(!word %in% undesirable_words) %>%
  filter(nchar(word) > 3) %>%
  count(chart_level, word, sort = TRUE) %>%
  ungroup() %>%
  bind_tf_idf(word, chart_level, n)

head(popular_tfidf_words)
```
As can be seen, the IDF and TF-IDF are 0 for the very common words (this is sorted by count(), 'n'). Will pipe this result into arrange() and descend by tf_idf to give us the highest scoring tf_idf words.
```{r}
top_popular_tfidf_words <- popular_tfidf_words %>%
  arrange(desc(tf_idf)) %>%
  mutate(word = factor(word, levels = rev(unique(word)))) %>%
  slice(seq_len(8)) %>%
  arrange(tf_idf) %>%
  mutate(row = row_number())

top_popular_tfidf_words %>%
  ggplot(aes(x = row, tf_idf)) +
    geom_col(show.legend = NULL) +
    labs(x = NULL, y = "TF-IDF") + 
    ggtitle("Important Words using TF-IDF by Chart Level") +
    theme_lyrics() +
    scale_x_continuous(  # This handles replacement of row 
      breaks = top_popular_tfidf_words$row, # notice need to reuse data frame
      labels = top_popular_tfidf_words$word) +
    coord_flip()
```

Will facet it by decade for added interest.

```{r}
tfidf_words_decade <- cohen %>%
  unnest_tokens(word, lyrics) %>%
  distinct() %>%
  filter(!word %in% undesirable_words) %>%
  filter(nchar(word) > 3) %>%
  count(decade, word, sort = TRUE) %>%
  ungroup() %>%
  bind_tf_idf(word, decade, n) %>%
  arrange(desc(tf_idf))

top_tfidf_words_decade <- tfidf_words_decade %>% 
  group_by(decade) %>% 
  slice(seq_len(8)) %>%
  ungroup() %>%
  arrange(decade, tf_idf) %>%
  mutate(row = row_number())

top_tfidf_words_decade %>%
  ggplot(aes(x = row, tf_idf, fill = decade)) +
    geom_col(show.legend = NULL) +
    labs(x = NULL, y = "TF-IDF") + 
    ggtitle("Important Words using TF-IDF by Decade") +
    theme_lyrics() +  
    facet_wrap(~decade, 
               ncol = 3, nrow = 2, 
               scales = "free") +
    scale_x_continuous(  # this handles replacement of row 
      breaks = top_tfidf_words_decade$row, # notice need to reuse data frame
      labels = top_tfidf_words_decade$word) +
    coord_flip()
```
Not too much insight from here -- sample sizes are possibly too small.

# Conclusions
This wraps up the exploratory data analysis section of the Leonard Cohen lyrics dataset. Will move on to some more advanced plotting and machine learning anlysis next.





